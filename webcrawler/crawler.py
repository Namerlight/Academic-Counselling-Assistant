from builtins import range, len
from urllib.request import Request, urlopen, ftperrors
from bs4 import BeautifulSoup
import mysql.connector
from mysql.connector import Error

# Generating the lists of data that will be recorded from the crawling
# Each list is of the same length and list[i] will contain the respective data for the 'i' ranked University
list_of_names = []
list_of_country = []
list_of_url_names = []
list_of_ranking = []
list_of_avg_price = []
list_of_ownership = []
list_of_student_pop = []
list_of_research = []


# Page_being_crawled contains the address of the web page being crawled for data
# Page is the address of the page, page_html is the extracted html of the page
# Requests that page, Mozilla Browser header to circumvent bot crawling protection
# The html is converted into the BeautifulSoup4 format (which is the scraping library) and stored as bs_format
# This data is in a table. We search for the keyword 'table' and place it in the variable named table
# We extract each row by finding the child tags of table, then each column by search for tag and attributes
# The second column gives us the name of the University, the third column gives us the Country.
# Append each name and country to the respective list, then removing the table headers
def page_crawl_unilist():
    page_being_crawled = 'https://www.topuniversities.com/student-info/choosing-university/worlds-top-100-universities'
    req = Request(page_being_crawled)
    req.add_header('User-Agent', 'Mozilla/5.0')
    page_html = urlopen(req)
    bs_format = BeautifulSoup(page_html, 'html.parser')
    table = bs_format.find('table')
    allrows = table.findChild()
    for row in allrows:
        second_column = row.find('td', attrs={'style': 'width: 455px;'})
        uni_names = second_column.text.strip()
        list_of_names.append(uni_names)
    for row2 in allrows:
        third_column = row2.find('td', attrs={'style': 'width: 124px;'})
        uni_countries = third_column.text.strip()
        list_of_country.append(uni_countries)
    del list_of_names[0]
    del list_of_country[0]


# This function converts the names of the Universities to a web address format that is used in topuniversities.com
# Removes from the name of the University the characters in the to_remove list, replaces spaces with '-'s
# Fixes other situational issues discovered during test runs
def generate_url_names():
    to_remove = {" of", "in ", " at", ",", " -", "'", " and", " &", "(", ")"}
    for name in list_of_names:
        nm = name.lower()
        for word in to_remove:
            nm = nm.replace(word, '')
        nm = nm.replace(' ste ', ' state ')
        nm = nm.replace(' ', '-')
        nm = nm.replace('universität', 'universitat')
        nm = nm.replace('münchen', 'munchen')
        list_of_url_names.append(nm)


# Looks up the list of urls generated by the previous function
# Uses the same method as earlier to extract the html document of the page into the variable bs_f
# Opens the individual pages of each University, which have all the same layout
# The data extracted include ranking, fees, research output, ownership and student population
# Not all of this data is given for all universities. Where it isn't, the slot in the final list is left blank
# The data is extracted into a list, uni_rank. This list contains 5 items
# list_iter iterates through this list, if conditions used to check if details of the Uni contains the specific data
# If it contains the data, the data is placed in the respective list (that were generated at the beginning)
# Else, it places '' to represent a blank and move to the next.
# unilist_iter iterates through the overall list of universities
def crawl_uni_info():
    unilist_iter = 0
    for url_uni_name in list_of_url_names:
        uni_page_url = 'https://www.topuniversities.com/universities/' + url_uni_name
        reqe = Request(uni_page_url)
        reqe.add_header('User-Agent', 'Mozilla/5.0')
        try:
            uni_page_html = urlopen(reqe)
        except ftperrors():
            print("", end='')
            continue
        uni_bs_format = BeautifulSoup(uni_page_html, 'html.parser')
        uni_stats_txt = uni_bs_format.find('div', class_='uni_stats').prettify()
        bs_f = BeautifulSoup(uni_stats_txt, 'html.parser')
        rank_label = bs_f.findAll('label')
        uni_rank = bs_f.findAll('div', attrs={'class': 'val'})
        list_iter = 0
        print(list_of_names[unilist_iter], end=' | ')
        print(list_of_country[unilist_iter], end=' | ')
        while list_iter < 5:
            list_iter = 0
            if rank_label[list_iter].text.strip() == 'QS Global World Ranking':
                list_of_ranking.append(uni_rank[list_iter].text.strip())
                print(uni_rank[list_iter].text.strip(), end=' | ')
                list_iter = 1
            if rank_label[list_iter].text.strip() == 'Average Fees (USD)':
                print(uni_rank[list_iter].text.strip(), end=' | ')
                list_of_avg_price.append(uni_rank[list_iter].text.strip())
                list_iter = 2
            else:
                print(" ", end=' | ')
                list_of_avg_price.append("")
            if rank_label[list_iter].text.strip() == 'Status':
                print(uni_rank[list_iter].text.strip(), end=' | ')
                list_of_ownership.append(uni_rank[list_iter].text.strip())
                list_iter = 3
            else:
                print(" ", end=' | ')
                list_of_ownership.append("")
            if rank_label[list_iter].text.strip() == 'Research Output':
                print(uni_rank[list_iter].text.strip(), end=' | ')
                list_of_research.append(uni_rank[list_iter].text.strip())
                list_iter = 4
            else:
                print(" ", end=' | ')
                list_of_research.append("")
            if rank_label[list_iter].text.strip() == 'Total Students':
                print(uni_rank[list_iter].text.strip(), end=' | ')
                list_of_student_pop.append(uni_rank[list_iter].text.strip())
                list_iter = 5
            else:
                print(" ", end=' | ')
                list_of_student_pop.append("")
        print('\n', end='')
        unilist_iter += 1


# Main. Executing the functions defined so far in order to fill up the lists with information
page_crawl_unilist()
generate_url_names()
crawl_uni_info()


# Connects to SQL database
# Database is named aca, user is root, no password
# Gives output on successful connection, error otherwise
try:
    connection = mysql.connector.connect(
        host='localhost',
        database='aca',
        user='root',
        password=''
    )
    if connection.is_connected():
            db_Info = connection.get_server_info()
            print("Connected to MySQL database... MySQL Server version on ", db_Info)
            cursor = connection.cursor()
            cursor.execute("select database()")
            record = cursor.fetchone()
            print("You're connected to", record)
except Error as e:
        print("Error while connecting to MySQL", e)

# Places cursor at head of database
cursor = connection.cursor()
print(cursor.rowcount)

# Removes all preexisting data
sql1 = "DELETE FROM universities"
cursor.execute(sql1)
connection.commit()

# Resets the auto-increment in SQL
sql2 = "ALTER TABLE universities AUTO_INCREMENT = 1"
cursor.execute(sql2)
connection.commit()

# Placing the crawled data into the SQL database by calliing each list, where they all contain respective information
for rw in range(0, len(list_of_names)-1):
    sql2 = "INSERT INTO universities(name, qs_ranking, research_output, status, total_student, average_fees, country) " \
           "VALUES (%s, %s, %s, %s, %s, %s, %s)"
    val = (list_of_names[rw], list_of_ranking[rw], list_of_research[rw], list_of_ownership[rw], list_of_student_pop[rw],
           list_of_avg_price[rw], list_of_country[rw])
    cursor.execute(sql2, val)
    connection.commit()
print(cursor.rowcount, "records inserted.")

